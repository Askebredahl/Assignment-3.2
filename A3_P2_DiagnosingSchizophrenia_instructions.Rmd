---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 3 - Part 2 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia. We then looked at whether we could replicate results from the previous literature.
We now want to know whether we can automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.
Again, remember that the dataset containst 7 studies and 3 languages. Feel free to only include Danish (Study 1-4) if you feel that adds too much complexity.

Issues to be discussed your report:
- Should you run the analysis on all languages/studies at the same time? 
- Choose your best acoustic feature from part 1. How well can you diagnose schizophrenia just using it?
- Identify the best combination of acoustic features to diagnose schizophrenia using logistic regression.
- Discuss the "classification" process: which methods are you using? Which confounds should you be aware of? What are the strength and limitation of the analysis?
- Bonus question: Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them. 
- Bonus Bonus question: It is possible combine the output of multiple  classification models to improve classification accuracy. For inspiration see,
https://machinelearningmastery.com/machine-learning-ensembles-with-r/
 The interested reader might also want to look up 'The BigChaos Solution to the Netflix Grand Prize'

## Learning objectives
- Learn the basics of classification in a machine learning framework
- Design, fit and report logistic regressions
- Apply feature selection techniques

### Let's start

We first want to build a logistic regression to see whether you can diagnose schizophrenia from your best acoustic feature. Let's use the full dataset and calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve). You need to think carefully as to how we should (or not) use study and subject ID.

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures. Alternatively, the groupdata2 and cvms package created by Ludvig are an easy solution. 

N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one datase, so to calculate overall performance.
N.N.N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?
N.N.N.N.B. A more advanced solution could rely on the tidymodels set of packages (warning: Time-consuming to learn as the documentation is sparse, but totally worth it)



```{r}
# load packages
pacman::p_load(pacman, tidyverse, tidymodels, groupdata2, yardstick, lme4, ModelMetrics, caret, cvms, e1071)



# load data. Schizodata.csv = merged_data
Schizodata <- read.csv("Merged__scaled_data.csv")

summary(Schizodata)
###

# build a logistic regression to diagnosis with the best acoustic feature. Use the full data set. Next, calculate performance metrics.
log_res_1 <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm") %>% 
  fit(Diagnosis.x ~ scaled_IQR, data = Schizodata)

# print assigned diagnosis as predicted by the model
log_class_1 <- log_res_1 %>%
  predict(new_data = Schizodata) %>% 
  pull(.pred_class)

# print the probabilities of having D1 for each participants
log_prop_1 <- log_res_1 %>%
  predict(new_data = Schizodata, type = "prob") %>% 
  pull(.pred_D1)

# create a dataframe containing: true diagnosis, logclass and log probability (for D1)
test_results <- 
  Schizodata %>% 
  select(Diagnosis.x) %>% 
  mutate(log_class_1) %>% 
  mutate(log_prop_1)

# calculate perfomance metrics
view(metrics(test_results, truth = Diagnosis.x, estimate = log_class_1))


# create a confusion matrix
yardstick::conf_mat(test_results, Diagnosis.x, log_class_1)

# calculate positive predicted value 
PPV <-  yardstick::ppv(test_results, Diagnosis.x, log_class_1, 0.52326)
PPV
# calculate negative predicted value
NPV <- yardstick::npv(test_results, Diagnosis.x, log_class_1, 0.47)
print(NPV)
# ROC-curve visualization
test_results %>%
  roc_curve(truth = Diagnosis.x, log_prop_1) %>% 
  autoplot()



###

summary(Schizodata)

# partition data into training and test data
set.seed(1)
df_list <- partition(Schizodata, p = 0.2, cat_col = "Diagnosis.x", id_col = "ID", list_out = T)
# append test and training to different dataframes
df_test <- df_list[[1]]
df_train <- df_list[[2]]
df_list

# create folds
folds <- fold(df_train, 5, cat_col = "Diagnosis.x", id_col = "ID")
folds


#log_res_2 <- glm(Diagnosis.x~ IQR, family = "binomial", folds)

log_res_2 <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm") %>% 
  fit(Diagnosis.x ~ scaled_IQR, data = folds)

cv1 <- cvms::cross_validate(folds, "Diagnosis.x ~ scaled_IQR", family = "binomial")


#log_on_test <- glm(Diagnosis.x ~ IQR, family = "binomial", df_test)

predict(log_res_2, df_test)

### PERFORMANCE METRICS FOR TRAINING DATA

# calculate performance metrics for how the model performs on the TRAINING set.
# print assigned diagnosis as predicted by the model
log_class_3 <- log_res_2 %>%
  predict(new_data = folds) %>%
  pull(.pred_class)


# print the probabilities of having D1 for each participants
log_prop_3 <- log_res_2 %>%
  predict(new_data = folds, type = "prob") %>% 
  pull(.pred_D1)


# create dataframe
test_results_3 <- 
  df_train %>% 
  select(Diagnosis.x) %>% 
  mutate(log_class_3) %>% 
  mutate(log_prop_3)

# calculate perfomance metrics
view(metrics(test_results_3, truth = Diagnosis.x, estimate = log_class_3))

# create a confusion matrix
yardstick::conf_mat(test_results_3, Diagnosis.x, log_class_3)

# calculate positive predicted value 
PPV3 <-  yardstick::ppv(test_results_3, Diagnosis.x, log_class_3, 0.52326)

# calculate negative predicted value
NPV3 <- yardstick::npv(test_results_3, Diagnosis.x, log_class_3, 0.47)

# ROC-curve visualization
test_results_3 %>%
  roc_curve(truth = Diagnosis.x, log_prop_3) %>% 
  autoplot()


### PERFORMANCE METRICS FOR TEST DATA

# calculate performance metrics for how the model performs on the test set.
# print assigned diagnosis as predicted by the model
log_class_2 <- log_res_2 %>%
  predict(new_data = df_test) %>%
  pull(.pred_class)


# print the probabilities of having D1 for each participants
log_prop_2 <- log_res_2 %>%
  predict(new_data = df_test, type = "prob") %>% 
  pull(.pred_D1)


# create dataframe
test_results_2 <- 
  df_test %>% 
  select(Diagnosis.x) %>% 
  mutate(log_class_2) %>% 
  mutate(log_prop_2)

# calculate perfomance metrics
view(metrics(test_results_2, truth = Diagnosis.x, estimate = log_class_2))

# create a confusion matrix
yardstick::conf_mat(test_results_2, Diagnosis.x, log_class_2)

# calculate positive predicted value 
PPV2 <-  yardstick::ppv(test_results_2, Diagnosis.x, log_class_2, 0.52326)

# calculate negative predicted value
NPV2 <- yardstick::npv(test_results_2, Diagnosis.x, log_class_2, 0.47)

# ROC-curve visualization
test_results_2 %>%
  roc_curve(truth = Diagnosis.x, log_prop_2) %>% 
  autoplot()


### Which combination of features has the best predictive power?

cv_IQR <- cvms::cross_validate(folds, 
  c("Diagnosis.x ~ scaled_IQR"),
    family = "binomial")

cv_speaktime <- cvms::cross_validate(folds, 
  c("Diagnosis.x ~ scaled_IQR + scaled_speak_time"),
    family = "binomial")

cv_speechrate <- cvms::cross_validate(folds, 
  c("Diagnosis.x ~ scaled_IQR + scaled_speech_rate"),
    family = "binomial")

cv_speechratetime <- cvms::cross_validate(folds, 
  c("Diagnosis.x ~ scaled_speak_time + scaled_speech_rate"),
    family = "binomial")

cv_speechtimerateiqr <- cvms::cross_validate(folds, 
  c("Diagnosis.x ~ scaled_IQR + scaled_speech_rate + scaled_speak_time"),
    family = "binomial")


# test speech_rate on the test data
log_res_4 <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm") %>% 
  fit(Diagnosis.x ~ scaled_IQR + scaled_speak_time + scaled_speech_rate, data = folds)




### PERFORMANCE METRICS FOR TEST DATA (iqr+speak_time)

# calculate performance metrics for how the model performs on the TRAINING set.
# print assigned diagnosis as predicted by the model
log_class_4 <- log_res_4 %>%
  predict(new_data = df_test) %>%
  pull(.pred_class)


# print the probabilities of having D1 for each participants
log_prop_4 <- log_res_4 %>%
  predict(new_data = df_test, type = "prob") %>% 
  pull(.pred_D1)


# create dataframe
test_results_4 <- 
  df_test %>% 
  select(Diagnosis.x) %>% 
  mutate(log_class_4) %>% 
  mutate(log_prop_4)

# calculate perfomance metrics
view(metrics(test_results_4, truth = Diagnosis.x, estimate = log_class_4))

# create a confusion matrix
yardstick::conf_mat(test_results_3, Diagnosis.x, log_class_3)

# calculate positive predicted value 
PPV3 <-  yardstick::ppv(test_results_3, Diagnosis.x, log_class_3, 0.52326)

# calculate negative predicted value
NPV3 <- yardstick::npv(test_results_3, Diagnosis.x, log_class_3, 0.47)

# ROC-curve visualization
test_results_3 %>%
  roc_curve(truth = Diagnosis.x, log_prop_3) %>% 
  autoplot()











#######

# create recipe. N.B. review the preprocessing steps
rec <- df_train %>% recipe(Diagnosis.x ~ IQR) %>% # predict diagnosis from all variables
  step_center(all_numeric()) %>% # normalize data to have a mean of 0
  step_scale(all_numeric()) %>% # scale so standard deviation is 1
  step_corr(all_numeric()) %>% # step_corr removes potential correlated variables
  prep(training = df_train) # creates a "recipe element" with training data


# extract the new scaled variables from rec
train_baked <- juice(rec) 
train_baked

# apply the training recipe to the test set
test_baked <- rec %>% bake(df_test)
#Even though the training set is part of the recipe, this does not affect baking the test set. The framework knows that it is dealing with a training and test set, and juicing() the recipe returns the training data set because this is specified in the end of the recipe (the prep(training = df_train)). If we insted deleted this line from the recipe and insted baked() the training set the same way we bake the test set, the result would be the same. 

# Create the logistic model and predicting diagnosis from IQR
log_res_2 <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm") %>% 
  fit(Diagnosis.x ~ IQR, data = train_baked)

log_res_2

# print assigned diagnosis as predicted by the model
log_class_2 <- log_res_2 %>%
  predict(new_data = test_baked) %>% 
  pull(.pred_class)


# print the probabilities of having D1 for each participants
log_prop_2 <- log_res_2 %>%
  predict(new_data = test_baked, type = "prob") %>% 
  pull(.pred_D1)

# create a dataframe containing: true diagnosis, logclass and log probability (for D1)
test_results_2 <- 
  test_baked %>% 
  select(Diagnosis.x) %>% 
  mutate(log_class_2) %>% 
  mutate(log_prop_2)

test_results_2


# get accuracy (and kappa if comparing several models)
metrics(test_results_2, truth = Diagnosis.x, estimate = log_class_2)


# create a ROC curve
test_results_2 %>% 
roc_curve(truth = Diagnosis.x, estimate = log_prop_2) %>% 
  autoplot()








#######

# time for multiple cross validation
## (use jacobs function for estimating the optimal amount of folds)
### V: number of partitions. strata: provides balanced folds in terms of diagnosis
folds <- vfold_cv(df_train, v = 5, repeats = 1, strata = Diagnosis.x)
# vfold_cv creates a dataframe containing a column called "splits". The values in this column are called "split objects". The individual cells does not contain single values, but whole data frames (look at the size of "folds").


folds <- folds %>% # use the previously created folds
  mutate(recipes = splits %>% # the column "recipes" now contains the split objects. (No idea why is is necesarry)
  map(prepper, recipe = rec), # map returns a list with elements that are run through a given function, prepper handles the "split objects" in folds
  train_data = splits %>% 
  map(training)) # return a list of the training data (see recipe)

head(folds$recipes)


# now we train the folds with a logistic regression is before
log_fit2 <- 
  logistic_reg() %>%
  set_mode("classification") %>% 
  set_engine("glm") 

folds <- folds %>%  
  mutate(log_fits = pmap(list(recipes, train_data), # pmap creates a list of multiple inputs. This list contains the split objects looped over the recipe and the model.
  ~ fit(log_fit2, formula(.x), data = bake(object = .x, new_data = .y))))


view(folds)
p_load(rsample)

splits <- folds$splits

predicted_log <- function(split, recipe, model) {
  baked_test <- bake(rec, new_data = testing(folds)) 
  tibble(
    actual = baked_test$Diagnosis.x,
    predicted = predict(log_fit2, baked_test) %>% pull(.pred_class),
    prob_diagnosis = predict(log_fit2, baked_test, type = "prob") %>% pull(.pred_Diagnosis.x)
  )
}



folds <- folds %>% 
  mutate(pred = pmap(list(baked_test$splits, baked_test$recipes, baked_test$log_fits), predicted_log))

  

```

